NLP-Modeling-and-Transformers

Large Language Models (LLMs) Projects


This repository contains a series of projects focused on implementing and experimenting with various language models and natural language processing (NLP) techniques. The projects range from classic N-gram models to modern Transformer architectures and advanced prompt engineering methods.

Projects Overview
Project : N-gram Models
Description: Tested various N-gram models on the Reuters dataset to explore their performance in predicting text sequences.
Project : Transformer Architecture Implementation and Machine Translation
Description: Implemented the Transformer model architecture from scratch.
Task: Trained the model on a machine translation task from German to English using the Multi30k dataset.
Additional Features: Implemented greedy decoding and beam search algorithms for translation.
Project : Text Summarization with HuggingFace BART
Description: Evaluated text summarization using three pre-trained Transformer models: BART, T5, and Pegasus, on the SAMSum dataset.
Results: Improved the ROUGE score of the BART model on the testing set from 28.7 to 37.5 through fine-tuning with Mixed Precision (AMP).
Project : Prompt Engineering and Few-Shot Learning with Flan-T5 for Dialogue Summarization
Description: Experimented with prompt engineering on Flan-T5 using the dialogsum dataset.
Approach: Used instructional prompts and pre-built T5 prompts.
Learning Methods: Explored zero-shot and few-shot learning to assess their impact on the relevance and coherence of the generated summaries.
